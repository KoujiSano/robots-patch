---
title: 'Nuxt Robots Features'
description: 'Nuxt Robots manages the robots crawling your site with minimal config and best practice defaults.'
navigation:
  title: 'Features'
---

Nuxt Robots manages the robots crawling your site with minimal config and best practice defaults.

## ðŸ¤– Robots.txt Config

Configuring the rules is as simple as adding a production robots.txt file to your project.

- [Config using Robots.txt](/robots/guides/robots-txt)

## ðŸ—¿ X-Robots-Tag Header, Meta Tag

Ensures pages that should not be indexed are not indexed with the following:
- `X-Robots-Tag` header
- `<meta name="robots" ...>` meta tag

Both enabled by default.

- [How it works](/robots/getting-started/how-it-works)

## ðŸ”’ Production only indexing

The module uses [Nuxt Site Config](/site-config/getting-started/background) to determine if the site is in production mode.

It will disables non-production environments from being indexed, avoiding duplicate content issues.

- [Environment Config](/robots/guides/disable-indexing)

## ðŸ”„ Easy and powerful configuration

Use route rules to easily target subsets of your site.
When you need even more control, use the runtime Nitro hooks to dynamically configure your robots rules.

- [Route Rules](/robots/guides/route-rules)
- [Nitro Hooks](/robots/nitro-api/nitro-hooks)

## ðŸŒŽ I18n Support

Will automatically fix any non-localised paths within your `allow` and `disallow` rules.

- [I18n Integration](/robots/integrations/i18n)
